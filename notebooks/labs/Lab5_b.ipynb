{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages(\"keras\")\n",
    "install.packages(\"tfruns\")\n",
    "install.packages(\"tfestimators\")\n",
    "install.packages(\"dslabs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_python(\"/home/creyesp/Projects/repos/r-course/venv/bin/python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Helper packages\n",
    "library(dplyr)         # for basic data wrangling\n",
    "\n",
    "# Modeling packages\n",
    "library(keras)         # for fitting DNNs\n",
    "library(tfruns)        # for additional grid search & model training functions\n",
    "\n",
    "# Modeling helper package - not necessary for reproducibility\n",
    "library(tfestimators)  # provides grid search & model training interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hiper-parametros\n",
    "* **Hidden Unit** - A function that takes an input (e.g. variables from data or the output of a previous layer) and performs a weighted sum followed by an application of the activation function.\n",
    "* **Activation Function** - Transformation of a hidden unit’s output, most often nonlinear. Modern activation functions usually are a member of the “LU” family and include Rectified Linear Units (ReLU), Parametric Rectified Linear Unit (PReLU), and Exponential Linear Units (ELU). For what it’s worth, I almost always stick with the standard ReLU, as I haven’t had much success with its more esoteric cousins.\n",
    "* **Layer** - A collection of hidden units at the same level in the graph. Takes a vector of inputs and produces a vector of outputs, where the size of the output is equal to the number of hidden units in the layer.\n",
    "* **Regularizaton** - Methods to combat overfitting, including dropout and penalties on the l1 and l2 norms of the weights.\n",
    "* **Loss Function** - The function to be minimized and the starting point for the backprop algorithm, usually determined by the specifics of the problem. Popular choices are binary cross entropy for 2 class classification, categorical cross entropy for multiclass classification, and mean-squared error for regression.\n",
    "* **Batch size** - Size of each mini-batch used in computing gradients for SGD.\n",
    "* **Optimizer** - Variant of SGD to use.\n",
    "* **Learning Rate** - Step size to be used in SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensible Defaults for Network Parameters\n",
    "\n",
    "Note these are not hard and fast rules and may be wildly inappropriate for your problem. These are ballparks figures for where I usually start when working on a new problem.\n",
    "\n",
    "* **Number of hidden units** - 128 - 256 to start until I get a sense of how complex the input-output relationship is, but can often go way higher to 1024 - 4096. If the network is overfitting badly on held out data, I will decrease and if it is under-fitting on the training data I will increase. I usually end up sticking with a single number that appears to be big enough and modulating the complexity of the network by adding or removing layers.\n",
    "* **Activation Function** - ReLU. This one’s pretty easy, I always go with ReLU first and usually end up sticking with it.\n",
    "* **Number of layers** - I usually start with 1 - 2 hidden layers and add and remove as I continue to explore. If it looks like adding more layers hurts performance on the training data, I will often see if adding residual connections between layers helps.\n",
    "* **Regularization** - A dropout rate of 0.5 between decently big layers (~ 1024 units) and a lower rate around ~0.2 if the layers are smaller. I don’t usually use l1 or l2 penalties, but if I do it’s something small like 1e-4. Typcially do not use batch normalization either, I haven’t found it to be useful and can make training weird if you use it in the wrong place.\n",
    "* **Optimizer** - Usually start with ADAM because it seems to work decently well off the shelf for a lot of problems. Once I hone in on a useful set of network parameters and architecture, I will experiment to see if SGD + nestrov momentum or RMSProp work better.\n",
    "* **Learning Rate** - 1e-3 to 1e-5 seems to be a good place to start. I usually start on the high-end and if the network has trouble learning (e.g. training loss doesn’t go down or bounces wildly), I’ll turn down the learning rate and try again.\n",
    "* **Batch size** - Depends on problem size and GPU memory, but I typically start with 64-128\n",
    "* **Epoch**:\n",
    "* **Data Scaling** - Center and scale each column. Neural nets like data be be near 0 and not have many large values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MNIST training data\n",
    "mnist <- dslabs::read_mnist()\n",
    "mnist_x <- mnist$train$images\n",
    "mnist_y <- mnist$train$labels\n",
    "\n",
    "# Rename columns and standardize feature values\n",
    "colnames(mnist_x) <- paste0(\"V\", 1:ncol(mnist_x))\n",
    "mnist_x <- mnist_x / 255\n",
    "\n",
    "# One-hot encode response\n",
    "mnist_y <- to_categorical(mnist_y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_cols = ncol(mnist_x)\n",
    "n_cols = 100\n",
    "model <- keras_model_sequential() %>%\n",
    "  # Network architecture\n",
    "  layer_dense(units = 128, activation = \"relu\", input_shape = n_cols) %>%\n",
    "  layer_dense(units = 64, activation = \"relu\") %>%\n",
    "  layer_dense(units = 10, activation = \"softmax\") %>%\n",
    "  \n",
    "  # Backpropagation\n",
    "  compile(\n",
    "    loss = 'categorical_crossentropy',\n",
    "    optimizer = optimizer_rmsprop(),\n",
    "    metrics = c('accuracy')\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "fit1 <- model %>%\n",
    "  fit(\n",
    "    x = mnist_x,\n",
    "    y = mnist_y,\n",
    "    epochs = 25,\n",
    "    batch_size = 128,\n",
    "    validation_split = 0.2,\n",
    "    verbose = FALSE\n",
    "  )\n",
    "fit1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(fit1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
